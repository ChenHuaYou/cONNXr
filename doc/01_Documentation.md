# 02 Documentation

## Introduction
### What is ONNX?
If you don't know about `onnx` you might want to read about it before. They have a nice [website](https://onnx.ai/) and great repositories with a lot of documentation to read about. Everything is open source, and really big companies in the industry are behind it (AMD, ARM, AWS, Nvidia, IBM) just to name a few. Here you can find a mix of official and non official related repositories:
* https://github.com/onnx/onnx
* https://github.com/onnx/onnx-r
* https://github.com/onnx/models
* https://github.com/owulveryck/onnx-go
* https://github.com/microsoft/onnxruntime

In short, `onnx` provides a **O**pen **N**eural **N**etwork **E**xchange format. This format, describes a huge set of operators, that can be mixed to create every type of machine learning model that you ever heard of, from a simple neural network to complex deep convolutional networks. Some examples of operators are: matrix multiplications, convolutions, adding, maxpool, sin, cosine, you name it! They provide a standardised set of operators [here](https://github.com/onnx/onnx/blob/master/docs/Operators.md). So we can say that `onnx` provides a layer of abstraction to ML models, which makes all frameworks compatible between them. Exporters are provided for a huge variety of frameworks (PyTorch, TensorFlow, Keras, Scikit-Learn) so if you want to convert a model from Keras to TensorFlow, you just have to use Keras exporter to export `Keras->ONNX` and then use the importer to import `ONNX-TensorFlow`.

In the following image, you can find an example on how a `onnx` model looks like. Its just a bunch of `nodes` that are connected between them to form a `graph`. Each node has an `operator` that takes some `inputs` with some specific `dimensions` and some `attributes` and calculates some `outputs`. This is how the inference is calculated, just forward propagating the input along every node until the last one is reached.

![mnist](/doc/img/mnist_example.png)

So this `.onnx` format can describe machine learning models through a set of nodes that contain specific operators connected among them. On top of that, they also offer a set of the so called "runtimes" [see link](https://onnx.ai/supported-tools.html). A runtime allows to run inference on a model, and they offer different ones in a wide variety of languages and hardware. Unfortunately, all the runtimes rely on modern versions of C/C++ with many abstractions and dependancies, which might be a no go for some specific projects. Here is where we come in.

### What is cONNXr
Well, now that you know about `onnx`, our project is just a runtime that runs inference on `onnx` models. The `c` means that is implemented in C language and the `r` means that its a runtime for `ONNX`. The only difference between this runtime and the others, is that this one is written in pure `C99` without any dependancy. This means that it should be able to compile with almost any compiler, no matter how old it is. Our goal is to enable embedded devices that doesn't have much resources or fancy features (like GPUs or any type of hardware accelerator) to run inference. No GPUs, no multithreading, no dependancies, just pure C code with the lowest possible footprint. Train your model in whatever ML framework you want, export it to `.onnx` and deploy it wherever you want.

You might also find this project useful if you work with some bare metal hardware with dedicated accelerators. If this is the case, you might find useful to reuse the architecture and replace the specific operators (functions) by your own ones.

Or perhaps you have a different use case. If this is the case, we would love to hear about it.


## File structure
The most relevant files and folders are the following ones:

* `include/src`: Source and header C files.
* `include/operators/onnx/`: Autogenerated header files for all operators and versions. There is one header file per operator, and within each file there is one function declaration for each data type. For example, for the operator `conv` version 11 there is one header file, that contains 3 function declarations (double, float and float16). Note that these functions are defined as extern and not implemented.
* `src/operators/implementation`: Here is where all the operators implementations are located. The functions that are defined here must use the prototype defined in `include/operators/onnx` mentioned above. See some examples.
* `src/operators/resolve`: This is autogenerated code, that resolves the mapping between an operator + data type and the function that should be executed.
* `src/operators/operator_set.c`: Autogenerated file that stores the relationship between an operator name (i.e. `Conv`) and the function that resolves its data type. Operator version is also taken into account.
* `scripts/onnx_generator`: Set of Python scripts that generates the header and C files existing in `include/operators/onnx`, `src/operators/resolve` and `src/operators/operator_set.c`.
* `src/test` and `include/test`: Test code divided into two levels: operator level and model level.
* `test`: Test vectors on operator and model level. Bunch of `.onnx` models and `.pb` files (input + expected output)

## Operators interface
All ONNX operators comply with the following interface. This struct contains all the data that an operator needs to run, like its inputs and attributes. In `operator_executer` it contains the pointer to the function that runs that specific node. For example, if node `i` contains a `Conv`, that `operator_executer` will point to the `conv` function defined in `src/operators/implementation`.

You can check more information about `Onnx__NodeProto` and `Onnx__TensorProto` structs in `onnx.pb-c.h`, which is a file that is generated from the ONNX common interface.

```c
struct node_context{
  Onnx__NodeProto     *onnx_node;
  Onnx__TensorProto  **inputs;
  Onnx__TensorProto  **outputs;
  operator_executer resolved_op;
  //int (*resolved_op)(node_context *ctx);
};
```

## Types and Structures
We can divide the types and structures that this repo uses into two:
* ONNX structures: These structures are exactly (or almost) the same as the ones that ONNX defines in its [onnx.proto](https://github.com/onnx/onnx/blob/master/onnx/onnx.proto). See `onnx.pb-c.h`.
* Custom structures: These structures are defined *ad hoc* for this project.


### Onnx__GraphProto
Defines the graph of a model, made of different nodes `Onnx__NodeProto`. See `onnx.pb-c.h`.

### Onnx__NodeProto
Information for a given node (a node has inputs and attributes and provides an output using a given operator). See `onnx.pb-c.h`.

### Onnx__TensorProto
One of the most important types that you will see, is the `Onnx__TensorProto`. It just defines a vector, an array, a matrix, or whatever you want to call it. It is quite convinient to use, because it is quite generic. You can store different types of values, with different sizes. As an example, lets say that we want to store a 3 dimension vector. In that case `n_dims=3` and `dims[0]`, `dims[1]`, `dims[2]` will store some values. Lets store some `float` values, so `data_type=ONNX__TENSOR_PROTO__DATA_TYPE__FLOAT `. In this case `n_float_data=dims[0]*dims[1]*dims[2]` and `float_data` will contain all the values in a single dimension array. The tensor has also a `name`.

```c
struct  _Onnx__TensorProto
{
  ProtobufCMessage base;
  size_t n_dims;
  int64_t *dims;

  protobuf_c_boolean has_data_type;
  int32_t data_type;
  Onnx__TensorProto__Segment *segment;

  size_t n_float_data;
  float *float_data;

  size_t n_int32_data;
  int32_t *int32_data;

  size_t n_string_data;
  ProtobufCBinaryData *string_data;

  size_t n_int64_data;
  int64_t *int64_data;

  char *name;
  char *doc_string;

  protobuf_c_boolean has_raw_data;
  ProtobufCBinaryData raw_data;

  size_t n_external_data;
  Onnx__StringStringEntryProto **external_data;

  protobuf_c_boolean has_data_location;
  Onnx__TensorProto__DataLocation data_location;

  size_t n_double_data;
  double *double_data;

  size_t n_uint64_data;
  uint64_t *uint64_data;
};
```

## Protocol Buffers
In order to convert from the ONNX interface defined in `onnx.proto`, the `protoc` library is used. `onnx` uses protocol buffers to serialize the models data. Note that `protobuf-c` is used to generate the `pb/onnx.pb-c.c` and `pb/onnx.pb-c.h`. Files are already provided, but you can generate it like this:

```
protoc --c_out=. onnx.proto
```

## Autogenerated code
In order to avoid boilerplate code, some `.c` and `.h` files are autogenerated from Python. You can find these Python scripts and more information under `scrips/onnx_generator` folder. These Python scripts take into account a given ONNX version and generates all the "infrastructure" from it.
