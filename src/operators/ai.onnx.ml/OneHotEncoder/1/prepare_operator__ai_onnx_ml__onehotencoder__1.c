//this file was generated by ../../../../../../scripts/onnx_generator/OperatorTemplate.py
#include "operator__ai_onnx_ml__onehotencoder__1.h"
#include "tracing.h"
#include "utils.h"

operator_status
prepare_operator__ai_onnx_ml__onehotencoder__1(
    node_context *ctx
)
{
    TRACE_ENTRY(1);

    TRACE_NODE(2, true, ctx->onnx_node);

    /* UNCOMMENT AS NEEDED */

    // Onnx__TensorProto *i_X = searchInputByName(ctx, 0);

    // TRACE_TENSOR(2, true, i_X);

    // Onnx__AttributeProto *a_cats_int64s = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"cats_int64s");
    // Onnx__AttributeProto *a_cats_strings = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"cats_strings");
    // Onnx__AttributeProto *a_zeros = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"zeros");

    // TRACE_ATTRIBUTE(2, a_cats_int64s, a_cats_int64s);
    // TRACE_ATTRIBUTE(2, a_cats_strings, a_cats_strings);
    // TRACE_ATTRIBUTE(2, a_zeros, a_zeros);

    // Onnx__TensorProto *o_Y = searchOutputByName(ctx, 0);

    /* ALLOCATE AND INITIALIZE CONTEXT HERE IF NEEDED */

    // size_t default_n_cats_int64s = ;
    // int64_t* default_cats_int64s = ;
    // size_t default_n_cats_strings = ;
    // char** default_cats_strings = ;
    // int64_t default_zeros = ;

    // context_operator__ai_onnx_ml__onehotencoder__1 *op_ctx = NULL;
    // op_ctx = malloc(sizeof(context_operator__ai_onnx_ml__onehotencoder__1));
    // TRACE_FATAL(0 , !op_ctx, "could not allocate executer_context");

    // op_ctx->n_cats_int64s = a_cats_int64s?a_cats_int64s->n_ints:default_n_cats_int64s;
    // op_ctx->cats_int64s = a_cats_int64s?a_cats_int64s->ints:ARRAYDUP(default_cats_int64s,default_n_cats_int64s);
    // TRACE_FATAL(0, !op_ctx->cats_int64s, "malloc failed");
    // op_ctx->n_cats_strings = a_cats_strings?a_cats_strings->n_strings:default_n_cats_strings;
    // if (a_cats_strings) {
    //     op_ctx->cats_strings = malloc(a_cats_strings->n_strings * sizeof(char*));
    //     TRACE_FATAL(0, !op_ctx->cats_strings, "malloc failed");
    //     for (int i = 0; i < a_cats_strings->n_strings; i++) { op_ctx->cats_strings[i] = strndup((char*)a_cats_strings->strings[i].data, a_cats_strings->strings[i].len); }
    // } else {
    //     op_ctx->cats_strings = a_cats_strings?a_cats_strings->strings:ARRAYDUP(default_cats_strings,default_n_cats_strings);
    //     TRACE_FATAL(0, !op_ctx->cats_strings, "malloc failed");
    // }
    // op_ctx->zeros = a_zeros?a_zeros->i:default_zeros;

    // TRACE_ARRAY(2, true, op_ctx->cats_int64s, , op_ctx->n_cats_int64s, "%" PRId64);
    // TRACE_ARRAY(2, true, op_ctx->cats_strings, , op_ctx->n_cats_strings, "\"%s\"");
    // TRACE_VAR(2, true, op_ctx->zeros, "%" PRId64);

    /* INITIALIZE OUTPUTS DATA_TYPE AND SHAPE HERE */


    /* MALLOC OUTPUT TENSORS HERE */

    // mallocTensorData(o_Y);

    // TRACE_TENSOR(2, true, o_Y);

    /* CHOOSE EXECUTER AND CONTEXT HERE */
    /* YOU MAY USE THE GENERATED RESOLVER */

    // ctx->executer = resolve_operator__ai_onnx_ml__onehotencoder__1(ctx);
    // ctx->executer_context = op_ctx;

    TRACE_EXIT(1);

    /* CHANGE RETURN CODE IF THIS PREPARER IS VALID */
    return OP_ENOSYS;
    // return OP_OK;
}