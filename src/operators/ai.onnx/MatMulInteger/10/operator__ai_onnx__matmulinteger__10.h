//this file was generated by ../../../../../../scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__MATMULINTEGER__10_H
# define OPERATOR_OPERATOR__AI_ONNX__MATMULINTEGER__10_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'MatMulInteger' version 10
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html.
 * The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.
 * 
 * Constraint T1:
 *   Constrain input A data type to 8-bit integer tensor.
 *   Allowed Types: tensor_int8, tensor_uint8
 * 
 * Constraint T2:
 *   Constrain input B data type to 8-bit integer tensor.
 *   Allowed Types: tensor_int8, tensor_uint8
 * 
 * Constraint T3:
 *   Constrain output Y data type as 32-bit integer tensor.
 *   Allowed Types: tensor_int32
 * Input T1 A:
 *   N-dimensional matrix A
 *   Allowed Types: tensor_int8, tensor_uint8
 * 
 * Input T2 B:
 *   N-dimensional matrix B
 *   Allowed Types: tensor_int8, tensor_uint8
 * 
 * Input T1 a_zero_point:
 *   Zero point tensor for input 'A'. It's optional and default value is 0. It
 *   could be a scalar or a 1-D tensor, which means a per-tensor or per-row
 *   quantization. If it's a 1-D tensor, its number of elements should be equal
 *   to the number of rows of input 'A'.
 *   Allowed Types: tensor_int8, tensor_uint8
 * 
 * Input T2 b_zero_point:
 *   Zero point tensor for input 'B'. It's optional and default value is 0. It
 *   could be a scalar or a 1-D tensor, which means a per-tensor or per-column
 *   quantization. If it's a 1-D tensor, its number of elements should be equal
 *   to the number of columns of input 'B'.
 *   Allowed Types: tensor_int8, tensor_uint8
 * Output T3 Y:
 *   Matrix multiply results from A * B
 *   Allowed Types: tensor_int32

 *
 * @since version 10
 *
 * @see io/onnx/onnx/defs/math/defs.cc:1634
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#MatMulInteger
 */

operator_status
prepare_operator__ai_onnx__matmulinteger__10(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__matmulinteger__10;

typedef struct {
// no attributes
} context_operator__ai_onnx__matmulinteger__10;

operator_executer
resolve_operator__ai_onnx__matmulinteger__10(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__matmulinteger__10(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__matmulinteger__10__T1_tensor_int8__T2_tensor_int8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__matmulinteger__10__T1_tensor_int8__T2_tensor_uint8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__matmulinteger__10__T1_tensor_uint8__T2_tensor_int8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__matmulinteger__10__T1_tensor_uint8__T2_tensor_uint8(
    node_context *ctx
);

# endif